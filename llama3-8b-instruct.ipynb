{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"77385c92b33f4f44b5a05a6757f6d867":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d3b19b1ea4ab44909302de823b7b36c6","IPY_MODEL_8d319d711720490fbbe1d41e3a49b0da","IPY_MODEL_82e2a14544fb4d26b084e5b47b47d608"],"layout":"IPY_MODEL_fcb66b975d9243079cd2a342c83758b0"}},"d3b19b1ea4ab44909302de823b7b36c6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ead307effcf64d53b42f0fc140cb3b55","placeholder":"​","style":"IPY_MODEL_9aa7d52bda984ef0bace25c9c0e656ce","value":"Loading checkpoint shards: 100%"}},"8d319d711720490fbbe1d41e3a49b0da":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c7fd47ede7c7497b9c5cffe95d3bd95d","max":4,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7394a942e2c14628a2d3117db17cf89c","value":4}},"82e2a14544fb4d26b084e5b47b47d608":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_255f80839c2149ba9a04709823f46bb6","placeholder":"​","style":"IPY_MODEL_e1e8bbaad7404c659505c3a176ce50ac","value":" 4/4 [01:21&lt;00:00, 17.70s/it]"}},"fcb66b975d9243079cd2a342c83758b0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ead307effcf64d53b42f0fc140cb3b55":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9aa7d52bda984ef0bace25c9c0e656ce":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c7fd47ede7c7497b9c5cffe95d3bd95d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7394a942e2c14628a2d3117db17cf89c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"255f80839c2149ba9a04709823f46bb6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e1e8bbaad7404c659505c3a176ce50ac":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2024-05-12T20:23:40.688365Z","iopub.execute_input":"2024-05-12T20:23:40.689014Z","iopub.status.idle":"2024-05-12T20:23:41.709098Z","shell.execute_reply.started":"2024-05-12T20:23:40.688983Z","shell.execute_reply":"2024-05-12T20:23:41.708144Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Sun May 12 20:23:41 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  Tesla P100-PCIE-16GB           Off | 00000000:00:04.0 Off |                    0 |\n| N/A   37C    P0              26W / 250W |      0MiB / 16384MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|  No running processes found                                                           |\n+---------------------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install accelerate","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SPJwcEAWA1-m","outputId":"6a179a19-3eed-49f0-9983-858041df797d","execution":{"iopub.status.busy":"2024-05-12T20:23:41.711125Z","iopub.execute_input":"2024-05-12T20:23:41.711476Z","iopub.status.idle":"2024-05-12T20:23:55.076000Z","shell.execute_reply.started":"2024-05-12T20:23:41.711409Z","shell.execute_reply":"2024-05-12T20:23:55.074717Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.29.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.22.2)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.2.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install bitsandbytes","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UUNJgesrBe1k","outputId":"e47fb510-82f2-4cf6-fab4-dbd6358e75ef","execution":{"iopub.status.busy":"2024-05-12T20:23:55.077621Z","iopub.execute_input":"2024-05-12T20:23:55.077990Z","iopub.status.idle":"2024-05-12T20:24:11.590890Z","shell.execute_reply.started":"2024-05-12T20:23:55.077952Z","shell.execute_reply":"2024-05-12T20:24:11.589950Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Collecting bitsandbytes\n  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nDownloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.43.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch","metadata":{"execution":{"iopub.status.busy":"2024-05-12T20:24:11.592983Z","iopub.execute_input":"2024-05-12T20:24:11.593285Z","iopub.status.idle":"2024-05-12T20:24:14.649930Z","shell.execute_reply.started":"2024-05-12T20:24:11.593256Z","shell.execute_reply":"2024-05-12T20:24:14.649165Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"torch.backends.cuda.enable_mem_efficient_sdp(False)\ntorch.backends.cuda.enable_flash_sdp(False)","metadata":{"execution":{"iopub.status.busy":"2024-05-12T20:24:14.651120Z","iopub.execute_input":"2024-05-12T20:24:14.651608Z","iopub.status.idle":"2024-05-12T20:24:14.656473Z","shell.execute_reply.started":"2024-05-12T20:24:14.651572Z","shell.execute_reply":"2024-05-12T20:24:14.655485Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import transformers\n\n\ntorch.cuda.empty_cache()\n\nmodel_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model_id,\n    model_kwargs={\"torch_dtype\": torch.bfloat16,\n                  \"load_in_4bit\": True},\n    device_map=\"auto\",\n    token='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'\n)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":84,"referenced_widgets":["77385c92b33f4f44b5a05a6757f6d867","d3b19b1ea4ab44909302de823b7b36c6","8d319d711720490fbbe1d41e3a49b0da","82e2a14544fb4d26b084e5b47b47d608","fcb66b975d9243079cd2a342c83758b0","ead307effcf64d53b42f0fc140cb3b55","9aa7d52bda984ef0bace25c9c0e656ce","c7fd47ede7c7497b9c5cffe95d3bd95d","7394a942e2c14628a2d3117db17cf89c","255f80839c2149ba9a04709823f46bb6","e1e8bbaad7404c659505c3a176ce50ac"]},"id":"heOWbdNN-hfy","outputId":"c55900b6-4beb-4284-e670-024800d50cc7","execution":{"iopub.status.busy":"2024-05-12T20:24:14.657629Z","iopub.execute_input":"2024-05-12T20:24:14.657894Z","iopub.status.idle":"2024-05-12T20:27:24.738493Z","shell.execute_reply.started":"2024-05-12T20:24:14.657871Z","shell.execute_reply":"2024-05-12T20:27:24.737604Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"2024-05-12 20:24:18.061157: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-12 20:24:18.061251: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-12 20:24:18.187073: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5c51691f65244efa85e0bd7fada5e29"}},"metadata":{}},{"name":"stderr","text":"The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df45099d41f84c7d820551944bd152c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0de7b4836d2844eb836ddcd6cb3ddf95"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ada76c22be2147eaa69e68f2d5d29800"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c28665c74fa462ab34b621baa7b7a82"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"109bbe777d374488b49cceb9d51572fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e819701c66b04425baded36760883db9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1cdced73afb4424688947101f898f221"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2d046ea61cb4a3484337002cd372341"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/51.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfbcc1b081674c54b8ad749cb4fdccf6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f78d8978c9564a18ba84c1ae70185487"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed08eb6bf4ee47f6a49404f5ea236f2a"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"!git clone https://github.com/yu432/diploma_work","metadata":{"execution":{"iopub.status.busy":"2024-05-12T20:27:24.739687Z","iopub.execute_input":"2024-05-12T20:27:24.740233Z","iopub.status.idle":"2024-05-12T20:27:26.359026Z","shell.execute_reply.started":"2024-05-12T20:27:24.740204Z","shell.execute_reply":"2024-05-12T20:27:26.357898Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Cloning into 'diploma_work'...\nremote: Enumerating objects: 16, done.\u001b[K\nremote: Counting objects: 100% (16/16), done.\u001b[K\nremote: Compressing objects: 100% (16/16), done.\u001b[K\nremote: Total 16 (delta 0), reused 16 (delta 0), pack-reused 0\u001b[K\nUnpacking objects: 100% (16/16), 850.75 KiB | 5.42 MiB/s, done.\n","output_type":"stream"}]},{"cell_type":"code","source":"!ls diploma_work/story_level_long/","metadata":{"execution":{"iopub.status.busy":"2024-05-12T20:27:26.360533Z","iopub.execute_input":"2024-05-12T20:27:26.360847Z","iopub.status.idle":"2024-05-12T20:27:27.375116Z","shell.execute_reply.started":"2024-05-12T20:27:26.360815Z","shell.execute_reply":"2024-05-12T20:27:27.373859Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"chehov_anyuta.txt   chehov_kot.txt\t    chehov_sobitie.txt\nchehov_detvora.txt  chehov_loshadinaya.txt  chehov_uchitel.txt\nchehov_gore.txt     chehov_nalim.txt\nchehov_grisha.txt   chehov_papasha.txt\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nfrom tqdm import tqdm\nimport gc\n\ndirectory = \"diploma_work/story_level_long/\"\n\nfiles = os.listdir(directory)\n\nfor file_name in files:\n    if file_name.endswith(\".txt\"):\n        file_path = os.path.join(directory, file_name)\n        with open(file_path, \"r\") as file:\n            file_content = file.read()\n            text = file_content\n            \n            torch.cuda.empty_cache()\n            gc.collect()\n            \n            messages = [\n                      {\"role\": \"system\", \"content\": \"You are a helpful assistant, respond the following text in Russian\"},\n                      {\"role\": \"user\", \"content\": 'Напиши суммаризацию этого текста, пиши по-русски, используй не больше 100 слов:\\n' + text},\n                  ]\n\n            prompt = pipeline.tokenizer.apply_chat_template(\n                  messages,\n                  tokenize=False,\n                  add_generation_prompt=True\n            )\n            \n            print(len(pipeline.tokenizer(messages[1]['content'])['input_ids']))\n\n            terminators = [\n              pipeline.tokenizer.eos_token_id,\n              pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n            ]\n            \n            outputs = pipeline(\n              prompt,\n              max_new_tokens = 1024,\n              eos_token_id=terminators,\n              do_sample=True,\n              temperature=0.25,\n              top_k=100,\n              top_p=0.98,\n            )\n            with open(file_name + \"_llama_3_8b_100_words_summary.txt\", 'w') as outfile:\n                outfile.write((outputs[0][\"generated_text\"][len(prompt):]))","metadata":{"execution":{"iopub.status.busy":"2024-05-12T20:43:15.116229Z","iopub.execute_input":"2024-05-12T20:43:15.116940Z","iopub.status.idle":"2024-05-12T20:48:04.056199Z","shell.execute_reply.started":"2024-05-12T20:43:15.116903Z","shell.execute_reply":"2024-05-12T20:48:04.055330Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"2174\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"3376\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"2598\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"1994\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"3211\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"2314\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"4521\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"3228\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"3562\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"3743\n","output_type":"stream"}]}]}