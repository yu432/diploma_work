{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"77385c92b33f4f44b5a05a6757f6d867":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d3b19b1ea4ab44909302de823b7b36c6","IPY_MODEL_8d319d711720490fbbe1d41e3a49b0da","IPY_MODEL_82e2a14544fb4d26b084e5b47b47d608"],"layout":"IPY_MODEL_fcb66b975d9243079cd2a342c83758b0"}},"d3b19b1ea4ab44909302de823b7b36c6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ead307effcf64d53b42f0fc140cb3b55","placeholder":"​","style":"IPY_MODEL_9aa7d52bda984ef0bace25c9c0e656ce","value":"Loading checkpoint shards: 100%"}},"8d319d711720490fbbe1d41e3a49b0da":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c7fd47ede7c7497b9c5cffe95d3bd95d","max":4,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7394a942e2c14628a2d3117db17cf89c","value":4}},"82e2a14544fb4d26b084e5b47b47d608":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_255f80839c2149ba9a04709823f46bb6","placeholder":"​","style":"IPY_MODEL_e1e8bbaad7404c659505c3a176ce50ac","value":" 4/4 [01:21&lt;00:00, 17.70s/it]"}},"fcb66b975d9243079cd2a342c83758b0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ead307effcf64d53b42f0fc140cb3b55":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9aa7d52bda984ef0bace25c9c0e656ce":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c7fd47ede7c7497b9c5cffe95d3bd95d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7394a942e2c14628a2d3117db17cf89c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"255f80839c2149ba9a04709823f46bb6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e1e8bbaad7404c659505c3a176ce50ac":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2024-05-13T23:29:52.572459Z","iopub.execute_input":"2024-05-13T23:29:52.572984Z","iopub.status.idle":"2024-05-13T23:29:53.680711Z","shell.execute_reply.started":"2024-05-13T23:29:52.572941Z","shell.execute_reply":"2024-05-13T23:29:53.679190Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Mon May 13 23:29:53 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  Tesla P100-PCIE-16GB           Off | 00000000:00:04.0 Off |                    0 |\n| N/A   33C    P0              32W / 250W |   3458MiB / 16384MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n+---------------------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install accelerate","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SPJwcEAWA1-m","outputId":"6a179a19-3eed-49f0-9983-858041df797d","execution":{"iopub.status.busy":"2024-05-13T23:29:53.683823Z","iopub.execute_input":"2024-05-13T23:29:53.684278Z","iopub.status.idle":"2024-05-13T23:30:06.176215Z","shell.execute_reply.started":"2024-05-13T23:29:53.684235Z","shell.execute_reply":"2024-05-13T23:30:06.175075Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.29.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.22.2)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.2.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install bitsandbytes","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UUNJgesrBe1k","outputId":"e47fb510-82f2-4cf6-fab4-dbd6358e75ef","execution":{"iopub.status.busy":"2024-05-13T23:30:06.177679Z","iopub.execute_input":"2024-05-13T23:30:06.177999Z","iopub.status.idle":"2024-05-13T23:30:18.647524Z","shell.execute_reply.started":"2024-05-13T23:30:06.177968Z","shell.execute_reply":"2024-05-13T23:30:18.646509Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Requirement already satisfied: bitsandbytes in /opt/conda/lib/python3.10/site-packages (0.43.1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch","metadata":{"execution":{"iopub.status.busy":"2024-05-13T23:30:18.648950Z","iopub.execute_input":"2024-05-13T23:30:18.649266Z","iopub.status.idle":"2024-05-13T23:30:18.653865Z","shell.execute_reply.started":"2024-05-13T23:30:18.649235Z","shell.execute_reply":"2024-05-13T23:30:18.652869Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"torch.backends.cuda.enable_mem_efficient_sdp(False)\ntorch.backends.cuda.enable_flash_sdp(False)","metadata":{"execution":{"iopub.status.busy":"2024-05-13T23:30:18.656239Z","iopub.execute_input":"2024-05-13T23:30:18.656563Z","iopub.status.idle":"2024-05-13T23:30:18.665315Z","shell.execute_reply.started":"2024-05-13T23:30:18.656538Z","shell.execute_reply":"2024-05-13T23:30:18.664426Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import transformers\n\n\ntorch.cuda.empty_cache()\n\nmodel_id = \"IlyaGusev/saiga_llama3_8b\"\n\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model_id,\n    model_kwargs={\"torch_dtype\": torch.bfloat16,\n                  \"load_in_8bit\": True},\n    device_map=\"auto\",\n    token='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'\n)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":84,"referenced_widgets":["77385c92b33f4f44b5a05a6757f6d867","d3b19b1ea4ab44909302de823b7b36c6","8d319d711720490fbbe1d41e3a49b0da","82e2a14544fb4d26b084e5b47b47d608","fcb66b975d9243079cd2a342c83758b0","ead307effcf64d53b42f0fc140cb3b55","9aa7d52bda984ef0bace25c9c0e656ce","c7fd47ede7c7497b9c5cffe95d3bd95d","7394a942e2c14628a2d3117db17cf89c","255f80839c2149ba9a04709823f46bb6","e1e8bbaad7404c659505c3a176ce50ac"]},"id":"heOWbdNN-hfy","outputId":"c55900b6-4beb-4284-e670-024800d50cc7","execution":{"iopub.status.busy":"2024-05-13T23:30:18.666261Z","iopub.execute_input":"2024-05-13T23:30:18.666510Z","iopub.status.idle":"2024-05-13T23:31:03.850360Z","shell.execute_reply.started":"2024-05-13T23:30:18.666488Z","shell.execute_reply":"2024-05-13T23:31:03.849439Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00671288759d49609e62e4aa3a8f2caa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/277 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d70884cf504948a59fafe052bb95fda9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/51.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fb4422c8b0948eeb3ed178bf154ee5b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d66c6f724f7425b9da307b06709bb74"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/446 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f2598373c9542e59d2cd43df230d268"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"!git clone https://github.com/yu432/diploma_work","metadata":{"execution":{"iopub.status.busy":"2024-05-13T23:31:03.851409Z","iopub.execute_input":"2024-05-13T23:31:03.851671Z","iopub.status.idle":"2024-05-13T23:31:05.936505Z","shell.execute_reply.started":"2024-05-13T23:31:03.851647Z","shell.execute_reply":"2024-05-13T23:31:05.935399Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Cloning into 'diploma_work'...\nremote: Enumerating objects: 89, done.\u001b[K\nremote: Counting objects: 100% (89/89), done.\u001b[K\nremote: Compressing objects: 100% (88/88), done.\u001b[K\nremote: Total 89 (delta 7), reused 83 (delta 1), pack-reused 0\u001b[K\nUnpacking objects: 100% (89/89), 906.66 KiB | 2.30 MiB/s, done.\n","output_type":"stream"}]},{"cell_type":"code","source":"!ls diploma_work/story_level_long/","metadata":{"execution":{"iopub.status.busy":"2024-05-13T23:31:05.938612Z","iopub.execute_input":"2024-05-13T23:31:05.939035Z","iopub.status.idle":"2024-05-13T23:31:06.984762Z","shell.execute_reply.started":"2024-05-13T23:31:05.938993Z","shell.execute_reply":"2024-05-13T23:31:06.983694Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"chehov_anyuta.txt   chehov_kot.txt\t    chehov_sobitie.txt\nchehov_detvora.txt  chehov_loshadinaya.txt  chehov_uchitel.txt\nchehov_gore.txt     chehov_nalim.txt\nchehov_grisha.txt   chehov_papasha.txt\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nfrom tqdm import tqdm\nimport gc\n\ndirectory = \"diploma_work/story_level_long/\"\n\nfiles = os.listdir(directory)\n\nfor file_name in files:\n    if file_name.endswith(\".txt\"):\n        file_path = os.path.join(directory, file_name)\n        with open(file_path, \"r\") as file:\n            if file_name == 'chehov_uchitel.txt':\n                continue\n            file_content = file.read()\n            text = file_content\n            \n            torch.cuda.empty_cache()\n            gc.collect()\n            \n            messages = [\n                      {\"role\": \"system\", \"content\": \"Ты ассистент и пишешь качественные суммаризации.\"},\n                      {\"role\": \"user\", \"content\": 'Напиши суммаризацию этого текста, пиши по-русски, используй не больше 100 слов:\\n' + text},\n                  ]\n\n            prompt = pipeline.tokenizer.apply_chat_template(\n                  messages,\n                  tokenize=False,\n                  add_generation_prompt=True\n            )\n            \n            print(len(pipeline.tokenizer(messages[1]['content'])['input_ids']))\n\n            terminators = [\n              pipeline.tokenizer.eos_token_id,\n              pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n            ]\n            \n            outputs = pipeline(\n              prompt,\n              max_new_tokens = 512,\n              eos_token_id=terminators,\n              do_sample=True,\n              temperature=0.25,\n              top_k=100,\n              top_p=0.98,\n            )\n            with open(file_name + \"_summary_saiga.txt\", 'w') as outfile:\n                outfile.write((outputs[0][\"generated_text\"][len(prompt):]))","metadata":{"execution":{"iopub.status.busy":"2024-05-13T23:48:32.888457Z","iopub.execute_input":"2024-05-13T23:48:32.889190Z","iopub.status.idle":"2024-05-14T00:00:48.969495Z","shell.execute_reply.started":"2024-05-13T23:48:32.889149Z","shell.execute_reply":"2024-05-14T00:00:48.968670Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"1994\n","output_type":"stream"},{"name":"stderr","text":"--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1100, in emit\n    msg = self.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 943, in format\n    return fmt.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 678, in format\n    record.message = record.getMessage()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n    msg = msg % self.args\nTypeError: not all arguments converted during string formatting\nCall stack:\n  File \"/opt/conda/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/opt/conda/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n    app.launch_new_instance()\n  File \"/opt/conda/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n    app.start()\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 701, in start\n    self.io_loop.start()\n  File \"/opt/conda/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\n    self.asyncio_loop.run_forever()\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n    self._run_once()\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n    handle._run()\n  File \"/opt/conda/lib/python3.10/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in dispatch_queue\n    await self.process_one()\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 523, in process_one\n    await dispatch(*args)\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 429, in dispatch_shell\n    await result\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n    reply_content = await reply_content\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 429, in do_execute\n    res = shell.run_cell(\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3051, in run_cell\n    result = self._run_cell(\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3106, in _run_cell\n    result = runner(coro)\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3311, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3493, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_34/2321716340.py\", line 39, in <module>\n    outputs = pipeline(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_generation.py\", line 240, in __call__\n    return super().__call__(text_inputs, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1167, in __call__\n    logger.warning_once(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/logging.py\", line 329, in warning_once\n    self.warning(*args, **kwargs)\nMessage: 'You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset'\nArguments: (<class 'UserWarning'>,)\n/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n","output_type":"stream"},{"name":"stdout","text":"2174\n3211\n2314\n3376\n3228\n3562\n2598\n3743\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}