{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Vikhr 0.5_it beta, chatml 004, 1052"
      ],
      "metadata": {
        "id": "paM-Uyf39Mxw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.36.2 -qq\n",
        "!pip install accelerate -qq\n",
        "!pip install SentencePiece -qq\n",
        "!pip install bitsandbytes -qq\n",
        "!pip install protobuf -qq"
      ],
      "metadata": {
        "id": "EvV3Engq9FLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "GlVI4VF7CG9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5EeNFU2-9ERi"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM,MistralForCausalLM\n",
        "import torch\n",
        "model = MistralForCausalLM.from_pretrained(\"Vikhrmodels/Vikhr-7B-instruct_0.4\",\n",
        "                                             device_map=\"auto\",\n",
        "                                             #attn_implementation=\"sdpa\",\n",
        "                                             #load_in_8bit_fp32_cpu_offload=True,\n",
        "                                             #token= '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',\n",
        "                                             load_in_4bit=True)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Vikhrmodels/Vikhr-7B-instruct_0.4\",use_fast=False)\n",
        "from transformers import  AutoTokenizer, pipeline\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/yu432/diploma_work"
      ],
      "metadata": {
        "id": "sd73nf5BIZnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "tph1HMxW4vcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import gc\n",
        "from transformers import TextStreamer, pipeline\n",
        "\n",
        "# Assuming 'model' and 'tokenizer' are already defined\n",
        "directory = \"diploma_work/story_level_long/\"\n",
        "files = os.listdir(directory)\n",
        "\n",
        "#streamer = TextStreamer(tokenizer)\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "output_directory = \"generated_summaries_100\"\n",
        "os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "for file_name in files:\n",
        "    if file_name.endswith(\".txt\"):\n",
        "        file_path = os.path.join(directory, file_name)\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "\n",
        "            file_content = file.read()\n",
        "            text = file_content\n",
        "\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "            print(len(tokenizer('Напиши суммаризацию этого текста, пиши по-русски, используй не больше 100 слов:\\n' + text)['input_ids']))\n",
        "            messages = [\n",
        "                      {\"role\": \"system\", \"content\": \"You are a helpful assistant, respond to the following text in Russian\"},\n",
        "                      {\"role\": \"user\", \"content\": 'Напиши суммаризацию этого текста, пиши по-русски, используй не больше 100 слов:\\n' + text},\n",
        "                  ]\n",
        "\n",
        "            prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "            outputs = pipe(prompt, max_new_tokens=2000, do_sample=True, use_cache=True, temperature=0.25, top_k=100, top_p=0.98, eos_token_id=79097)\n",
        "\n",
        "            generated_text = outputs[0]['generated_text'][len(prompt):].strip()\n",
        "\n",
        "            # Write the generated summary to a separate file\n",
        "            output_file_path = os.path.join(output_directory, f\"generated_100_{file_name[:-4]}.txt\")\n",
        "            with open(output_file_path, \"w\", encoding=\"utf-8\") as output_file:\n",
        "                output_file.write(generated_text)\n"
      ],
      "metadata": {
        "id": "_xwocDUk9YbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qALfXlfjHUpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "2q0gP0i6HYvR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}