{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"77385c92b33f4f44b5a05a6757f6d867":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d3b19b1ea4ab44909302de823b7b36c6","IPY_MODEL_8d319d711720490fbbe1d41e3a49b0da","IPY_MODEL_82e2a14544fb4d26b084e5b47b47d608"],"layout":"IPY_MODEL_fcb66b975d9243079cd2a342c83758b0"}},"d3b19b1ea4ab44909302de823b7b36c6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ead307effcf64d53b42f0fc140cb3b55","placeholder":"​","style":"IPY_MODEL_9aa7d52bda984ef0bace25c9c0e656ce","value":"Loading checkpoint shards: 100%"}},"8d319d711720490fbbe1d41e3a49b0da":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c7fd47ede7c7497b9c5cffe95d3bd95d","max":4,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7394a942e2c14628a2d3117db17cf89c","value":4}},"82e2a14544fb4d26b084e5b47b47d608":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_255f80839c2149ba9a04709823f46bb6","placeholder":"​","style":"IPY_MODEL_e1e8bbaad7404c659505c3a176ce50ac","value":" 4/4 [01:21&lt;00:00, 17.70s/it]"}},"fcb66b975d9243079cd2a342c83758b0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ead307effcf64d53b42f0fc140cb3b55":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9aa7d52bda984ef0bace25c9c0e656ce":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c7fd47ede7c7497b9c5cffe95d3bd95d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7394a942e2c14628a2d3117db17cf89c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"255f80839c2149ba9a04709823f46bb6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e1e8bbaad7404c659505c3a176ce50ac":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2024-05-12T23:57:21.675679Z","iopub.execute_input":"2024-05-12T23:57:21.676479Z","iopub.status.idle":"2024-05-12T23:57:22.676099Z","shell.execute_reply.started":"2024-05-12T23:57:21.676450Z","shell.execute_reply":"2024-05-12T23:57:22.674946Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Sun May 12 23:57:22 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  Tesla P100-PCIE-16GB           Off | 00000000:00:04.0 Off |                    0 |\n| N/A   36C    P0              27W / 250W |      0MiB / 16384MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|  No running processes found                                                           |\n+---------------------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install accelerate","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SPJwcEAWA1-m","outputId":"6a179a19-3eed-49f0-9983-858041df797d","execution":{"iopub.status.busy":"2024-05-12T23:57:22.678598Z","iopub.execute_input":"2024-05-12T23:57:22.679442Z","iopub.status.idle":"2024-05-12T23:57:36.238960Z","shell.execute_reply.started":"2024-05-12T23:57:22.679402Z","shell.execute_reply":"2024-05-12T23:57:36.237739Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.29.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.22.2)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.2.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install bitsandbytes","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UUNJgesrBe1k","outputId":"e47fb510-82f2-4cf6-fab4-dbd6358e75ef","execution":{"iopub.status.busy":"2024-05-12T23:57:36.241048Z","iopub.execute_input":"2024-05-12T23:57:36.241441Z","iopub.status.idle":"2024-05-12T23:57:53.069420Z","shell.execute_reply.started":"2024-05-12T23:57:36.241398Z","shell.execute_reply":"2024-05-12T23:57:53.068461Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting bitsandbytes\n  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nDownloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.43.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch","metadata":{"execution":{"iopub.status.busy":"2024-05-12T23:57:53.071817Z","iopub.execute_input":"2024-05-12T23:57:53.072130Z","iopub.status.idle":"2024-05-12T23:57:56.298552Z","shell.execute_reply.started":"2024-05-12T23:57:53.072102Z","shell.execute_reply":"2024-05-12T23:57:56.297701Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"torch.backends.cuda.enable_mem_efficient_sdp(False)\ntorch.backends.cuda.enable_flash_sdp(False)","metadata":{"execution":{"iopub.status.busy":"2024-05-12T23:57:56.299686Z","iopub.execute_input":"2024-05-12T23:57:56.300062Z","iopub.status.idle":"2024-05-12T23:57:56.305535Z","shell.execute_reply.started":"2024-05-12T23:57:56.300036Z","shell.execute_reply":"2024-05-12T23:57:56.304710Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import transformers\n\n\ntorch.cuda.empty_cache()\n\nmodel_id = \"IlyaGusev/saiga_llama3_8b\"\n\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model_id,\n    model_kwargs={\"torch_dtype\": torch.bfloat16,\n                  \"load_in_4bit\": True},\n    device_map=\"auto\",\n    token='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'\n)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":84,"referenced_widgets":["77385c92b33f4f44b5a05a6757f6d867","d3b19b1ea4ab44909302de823b7b36c6","8d319d711720490fbbe1d41e3a49b0da","82e2a14544fb4d26b084e5b47b47d608","fcb66b975d9243079cd2a342c83758b0","ead307effcf64d53b42f0fc140cb3b55","9aa7d52bda984ef0bace25c9c0e656ce","c7fd47ede7c7497b9c5cffe95d3bd95d","7394a942e2c14628a2d3117db17cf89c","255f80839c2149ba9a04709823f46bb6","e1e8bbaad7404c659505c3a176ce50ac"]},"id":"heOWbdNN-hfy","outputId":"c55900b6-4beb-4284-e670-024800d50cc7","execution":{"iopub.status.busy":"2024-05-12T23:57:56.306947Z","iopub.execute_input":"2024-05-12T23:57:56.307230Z","iopub.status.idle":"2024-05-13T00:09:52.909283Z","shell.execute_reply.started":"2024-05-12T23:57:56.307197Z","shell.execute_reply":"2024-05-13T00:09:52.908324Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"2024-05-12 23:57:59.802413: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-12 23:57:59.802542: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-12 23:57:59.951459: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/689 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e94814ad22bb4e9cbee5485f9b5b71cb"}},"metadata":{}},{"name":"stderr","text":"The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd5b4a466d5545e0b56db649bfb86a91"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0545e3370306432aa6dde7c1c6b37541"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cfad3c54956744d8a5d140538ffc7969"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04c9df6d36ed4ad2bf50e2b87a20b7f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"694dc453d969416fad058318e4a8176f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e91a7fd73e4444749ae83d5a8aabcce3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc86474c74d64c6e8134954e73c4c83d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/277 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7157fb518c04f45888a96eff019e1b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/51.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd36cb48021841298987388265b4a6b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f8c6eac898f400dbe2323c803d258cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/446 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab3d338cca824384aab68ade2368a414"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"!git clone https://github.com/yu432/diploma_work","metadata":{"execution":{"iopub.status.busy":"2024-05-13T00:09:58.787672Z","iopub.execute_input":"2024-05-13T00:09:58.788339Z","iopub.status.idle":"2024-05-13T00:10:01.115047Z","shell.execute_reply.started":"2024-05-13T00:09:58.788310Z","shell.execute_reply":"2024-05-13T00:10:01.114060Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Cloning into 'diploma_work'...\nremote: Enumerating objects: 41, done.\u001b[K\nremote: Counting objects: 100% (41/41), done.\u001b[K\nremote: Compressing objects: 100% (41/41), done.\u001b[K\nremote: Total 41 (delta 4), reused 37 (delta 0), pack-reused 0\u001b[K\nUnpacking objects: 100% (41/41), 868.15 KiB | 2.20 MiB/s, done.\n","output_type":"stream"}]},{"cell_type":"code","source":"!ls diploma_work/story_level_long/","metadata":{"execution":{"iopub.status.busy":"2024-05-13T00:10:04.769603Z","iopub.execute_input":"2024-05-13T00:10:04.770115Z","iopub.status.idle":"2024-05-13T00:10:05.758404Z","shell.execute_reply.started":"2024-05-13T00:10:04.770082Z","shell.execute_reply":"2024-05-13T00:10:05.757393Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"chehov_anyuta.txt   chehov_kot.txt\t    chehov_sobitie.txt\nchehov_detvora.txt  chehov_loshadinaya.txt  chehov_uchitel.txt\nchehov_gore.txt     chehov_nalim.txt\nchehov_grisha.txt   chehov_papasha.txt\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nfrom tqdm import tqdm\nimport gc\n\ndirectory = \"diploma_work/story_level_long/\"\n\nfiles = os.listdir(directory)\n\nfor file_name in files:\n    if file_name.endswith(\".txt\"):\n        file_path = os.path.join(directory, file_name)\n        with open(file_path, \"r\") as file:\n            file_content = file.read()\n            text = file_content\n            \n            torch.cuda.empty_cache()\n            gc.collect()\n            \n            messages = [\n                      {\"role\": \"system\", \"content\": \"You are a helpful assistant, respond the following text in Russian\"},\n                      {\"role\": \"user\", \"content\": 'Напиши суммаризацию этого текста, пиши по-русски, используй не больше 1000 слов:\\n' + text},\n                  ]\n\n            prompt = pipeline.tokenizer.apply_chat_template(\n                  messages,\n                  tokenize=False,\n                  add_generation_prompt=True\n            )\n            \n            print(len(pipeline.tokenizer(messages[1]['content'])['input_ids']))\n\n            terminators = [\n              pipeline.tokenizer.eos_token_id,\n              pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n            ]\n            \n            outputs = pipeline(\n              prompt,\n              max_new_tokens = 3072,\n              eos_token_id=terminators,\n              do_sample=True,\n              temperature=0.25,\n              top_k=100,\n              top_p=0.98,\n            )\n            with open(file_name + \"1000_saiga_llama_3_8b_words_summary.txt\", 'w') as outfile:\n                outfile.write((outputs[0][\"generated_text\"][len(prompt):]))","metadata":{"execution":{"iopub.status.busy":"2024-05-13T00:21:20.506452Z","iopub.execute_input":"2024-05-13T00:21:20.506826Z","iopub.status.idle":"2024-05-13T00:34:39.785924Z","shell.execute_reply.started":"2024-05-13T00:21:20.506799Z","shell.execute_reply":"2024-05-13T00:34:39.785083Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"3377\n3563\n3212\n3229\n3744\n2315\n2599\n1995\n2175\n4522\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}